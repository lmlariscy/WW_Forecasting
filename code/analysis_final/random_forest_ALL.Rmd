---
title: "Random Forest - Detection Frequency"
author: "Leah Lariscy"
output: html_document
---

# Load Packages

```{r}
knitr::opts_chunk$set(message=F)
```

```{r}
library(tidyverse)
library(here)
library(tidymodels)
library(ggpubr)
library(tsibble)
library(ingredients)
library(RColorBrewer)
```

# Load Data

```{r}
# n = 6 (original data)
data_n6 <- readRDS(here("data/processed_data/wbe_covid_n6_week.rds")) %>% drop_na()

# n = 5
data_n5 <- readRDS(here("data/processed_data/wbe_covid_n5_week.rds")) %>% drop_na()

# n = 4
data_n4 <- readRDS(here("data/processed_data/wbe_covid_n4_week.rds")) %>% drop_na()

# n = 3
data_n3 <- readRDS(here("data/processed_data/wbe_covid_n3_week.rds")) %>% drop_na()

# n = 2
data_n2 <- readRDS(here("data/processed_data/wbe_covid_n2_week.rds")) %>% drop_na()

# n = 1
data_n1 <- readRDS(here("data/processed_data/wbe_covid_n1_week.rds")) %>% drop_na()
```

# Split data

```{r}
data_n6_train <- data_n6 %>% head(n = 75)
data_n6_test <- data_n6 %>% tail(n = 50)

data_n5_train <- data_n5 %>% head(n = 75)
data_n5_test <- data_n5 %>% tail(n = 50)

data_n4_train <- data_n4 %>% head(n = 75)
data_n4_test <- data_n4 %>% tail(n = 50)

data_n3_train <- data_n3 %>% head(n = 75)
data_n3_test <- data_n3 %>% tail(n = 50)

data_n2_train <- data_n2 %>% head(n = 75)
data_n2_test <- data_n2 %>% tail(n = 50)

data_n1_train <- data_n1 %>% head(n = 75)
data_n1_test <- data_n1 %>% tail(n = 50)
```

# Model tuning

## Define model

```{r}
#define random forest model
rf_model <- rand_forest(
  mtry = tune(), 
  trees = 1000, 
  min_n = tune()) %>% 
  set_engine("ranger", importance = "permutation") %>% 
  set_mode("regression") 

#define null model
null_model <- null_model() %>% 
  set_engine("parsnip") %>% 
  set_mode("regression")
```

## Define recipes

```{r}
all_n6_recipe <- recipe(log10_cases ~ 
              A_N1_POS+A_N2_POS+B_N1_POS+B_N2_POS+C_N1_POS+C_N2_POS+
              A_N1+A_N2+B_N1+B_N2+C_N1+C_N2+week,
              data = data_n6_train) %>% 
              update_role(week, new_role = "ID") 

all_n5_recipe <- recipe(log10_cases ~ 
              A_N1_POS+A_N2_POS+B_N1_POS+B_N2_POS+C_N1_POS+C_N2_POS+
              A_N1+A_N2+B_N1+B_N2+C_N1+C_N2+week,
              data = data_n5_train) %>% 
              update_role(week, new_role = "ID") 

all_n4_recipe <- recipe(log10_cases ~ 
              A_N1_POS+A_N2_POS+B_N1_POS+B_N2_POS+C_N1_POS+C_N2_POS+
              A_N1+A_N2+B_N1+B_N2+C_N1+C_N2+week,
              data = data_n4_train) %>% 
              update_role(week, new_role = "ID")

all_n3_recipe <- recipe(log10_cases ~ 
              A_N1_POS+A_N2_POS+B_N1_POS+B_N2_POS+C_N1_POS+C_N2_POS+
              A_N1+A_N2+B_N1+B_N2+C_N1+C_N2+week,
              data = data_n3_train) %>% 
              update_role(week, new_role = "ID")

all_n2_recipe <- recipe(log10_cases ~ 
              A_N1_POS+A_N2_POS+B_N1_POS+B_N2_POS+C_N1_POS+C_N2_POS+
              A_N1+A_N2+B_N1+B_N2+C_N1+C_N2+week,
              data = data_n2_train) %>% 
              update_role(week, new_role = "ID")

all_n1_recipe <- recipe(log10_cases ~ 
              A_N1_POS+A_N2_POS+B_N1_POS+B_N2_POS+C_N1_POS+C_N2_POS+
              A_N1+A_N2+B_N1+B_N2+C_N1+C_N2+week,
              data = data_n1_train) %>% 
              update_role(week, new_role = "ID")
```

## Create workflows

```{r}
#workflow for all predictors model (to be tuned)
all_workflow_n6 <- workflow() %>% 
  add_model(rf_model) %>% 
  add_recipe(all_n6_recipe)

#workflow for null models
all_null_workflow_n6 <- workflow() %>% 
  add_model(null_model) %>% 
  add_recipe(all_n6_recipe)

all_null_workflow_n5 <- workflow() %>% 
  add_model(null_model) %>% 
  add_recipe(all_n5_recipe)

all_null_workflow_n4 <- workflow() %>% 
  add_model(null_model) %>% 
  add_recipe(all_n4_recipe)

all_null_workflow_n3 <- workflow() %>% 
  add_model(null_model) %>% 
  add_recipe(all_n3_recipe)

all_null_workflow_n2 <- workflow() %>% 
  add_model(null_model) %>% 
  add_recipe(all_n2_recipe)

all_null_workflow_n1 <- workflow() %>% 
  add_model(null_model) %>% 
  add_recipe(all_n1_recipe)
```

## Set up cross-validation for tuning

```{r}
set.seed(13)
folds_n6 <- vfold_cv(data_n6_train)
```

## Define tuning grid

```{r}
doParallel::registerDoParallel()

set.seed(345)
tune_rf_3 <- tune_grid(
  all_workflow_n6,
  resamples = folds_n6,
  grid = 20)
```

```{r}
#check RMSE
tune_rf_3 %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "RMSE")
```

```{r}
#check RSQ
tune_rf_3 %>%
  collect_metrics() %>%
  filter(.metric == "rsq") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "R-Squared")
```

## Select the best model

```{r}
#select the best model based on RMSE
best_rmse_3 <- tune_rf_3 %>%
  select_best(metric = "rmse")

final_rf_3 <- finalize_model(rf_model, best_rmse_3)

final_rf_3
```

# Model fitting & CV

## Create final workflows

```{r}
all_n6_final_wf <- workflow() %>% 
  add_model(final_rf_3) %>% 
  add_recipe(all_n6_recipe)

all_n5_final_wf <- workflow() %>% 
  add_model(final_rf_3) %>% 
  add_recipe(all_n5_recipe)

all_n4_final_wf <- workflow() %>% 
  add_model(final_rf_3) %>% 
  add_recipe(all_n4_recipe)

all_n3_final_wf <- workflow() %>% 
  add_model(final_rf_3) %>% 
  add_recipe(all_n3_recipe)

all_n2_final_wf <- workflow() %>% 
  add_model(final_rf_3) %>% 
  add_recipe(all_n2_recipe)

all_n1_final_wf <- workflow() %>% 
  add_model(final_rf_3) %>% 
  add_recipe(all_n1_recipe)
```

## n = 6

```{r}
#fit model to data
set.seed(13)
fit_all_n6 <- all_n6_final_wf %>% 
  fit(data = data_n6_train)

#create folds for cross validation
set.seed(13)
folds_all_n6 <- vfold_cv(data_n6_train, 
                        v = nrow(data_n6_train))

#run cross validation
set.seed(13)
cv_all_n6 <- fit_resamples(all_n6_final_wf, resamples = folds_all_n6)
cv_all_n6_metrics <- collect_metrics(cv_all_n6,summarize = F)
all_n6_metrics_sum <- collect_metrics(cv_all_n6,summarize = T)

#check cross validation metrics
all_n6_metrics_sum
```

### Null model

```{r}
#create folds for cross validation
set.seed(13)
folds_all_n6_null <- vfold_cv(data_n6_train, 
                        v = nrow(data_n6_train))

#run cross validation
set.seed(13)
cv_all_n6_null <- fit_resamples(all_null_workflow_n6, 
                                resamples = folds_all_n6_null)
cv_all_n6_metrics_null <- collect_metrics(cv_all_n6_null ,summarize = F)
all_n6_metrics_null_sum <- collect_metrics(cv_all_n6_null ,summarize = T)

#check cross validation metrics
all_n6_metrics_null_sum
```

## n = 5

```{r}
#fit model to data
set.seed(13)
fit_all_n5 <- all_n5_final_wf %>% 
  fit(data = data_n5_train)

#create folds for cross validation
set.seed(13)
folds_all_n5 <- vfold_cv(data_n5_train, 
                        v = nrow(data_n5_train))

#run cross validation
set.seed(13)
cv_all_n5 <- fit_resamples(all_n5_final_wf, resamples = folds_all_n5)
cv_all_n5_metrics <- collect_metrics(cv_all_n5,summarize = F)
all_n5_metrics_sum <- collect_metrics(cv_all_n5,summarize = T)

#check cross validation metrics
all_n5_metrics_sum
```

### Null model

```{r}
#create folds for cross validation
set.seed(13)
folds_all_n5_null <- vfold_cv(data_n5_train, 
                        v = nrow(data_n5_train))

#run cross validation
set.seed(13)
cv_all_n5_null <- fit_resamples(all_null_workflow_n5, 
                                resamples = folds_all_n5_null)
cv_all_n5_metrics_null <- collect_metrics(cv_all_n5_null ,summarize = F)
all_n5_metrics_null_sum <- collect_metrics(cv_all_n5_null ,summarize = T)

#check cross validation metrics
all_n5_metrics_null_sum
```

## n = 4

```{r}
#fit model to data
set.seed(13)
fit_all_n4 <- all_n4_final_wf %>% 
  fit(data = data_n4_train)

#create folds for cross validation
set.seed(13)
folds_all_n4 <- vfold_cv(data_n4_train, 
                        v = nrow(data_n4_train))

#run cross validation
set.seed(13)
cv_all_n4 <- fit_resamples(all_n4_final_wf, resamples = folds_all_n4)
cv_all_n4_metrics <- collect_metrics(cv_all_n4,summarize = F)
all_n4_metrics_sum <- collect_metrics(cv_all_n4,summarize = T)

#check cross validation metrics
all_n4_metrics_sum
```

### Null model

```{r}
#create folds for cross validation
set.seed(13)
folds_all_n4_null <- vfold_cv(data_n4_train, 
                        v = nrow(data_n4_train))

#run cross validation
set.seed(13)
cv_all_n4_null <- fit_resamples(all_null_workflow_n4, 
                                resamples = folds_all_n4_null)
cv_all_n4_metrics_null <- collect_metrics(cv_all_n4_null ,summarize = F)
all_n4_metrics_null_sum <- collect_metrics(cv_all_n4_null ,summarize = T)

#check cross validation metrics
all_n4_metrics_null_sum
```

## n = 3

```{r}
#fit model to data
set.seed(13)
fit_all_n3 <- all_n3_final_wf %>% 
  fit(data = data_n3_train)

#create folds for cross validation
set.seed(13)
folds_all_n3 <- vfold_cv(data_n3_train, 
                        v = nrow(data_n3_train))

#run cross validation
set.seed(13)
cv_all_n3 <- fit_resamples(all_n3_final_wf, resamples = folds_all_n3)
cv_all_n3_metrics <- collect_metrics(cv_all_n3,summarize = F)
all_n3_metrics_sum <- collect_metrics(cv_all_n3,summarize = T)

#check cross validation metrics
all_n3_metrics_sum
```

### Null model

```{r}
#create folds for cross validation
set.seed(13)
folds_all_n3_null <- vfold_cv(data_n3_train, 
                        v = nrow(data_n3_train))

#run cross validation
set.seed(13)
cv_all_n3_null <- fit_resamples(all_null_workflow_n3, 
                                resamples = folds_all_n3_null)
cv_all_n3_metrics_null <- collect_metrics(cv_all_n3_null ,summarize = F)
all_n3_metrics_null_sum <- collect_metrics(cv_all_n3_null ,summarize = T)

#check cross validation metrics
all_n3_metrics_null_sum
```

## n = 2

```{r}
#fit model to data
set.seed(13)
fit_all_n2 <- all_n2_final_wf %>% 
  fit(data = data_n2_train)

#create folds for cross validation
set.seed(13)
folds_all_n2 <- vfold_cv(data_n2_train, 
                        v = nrow(data_n2_train))

#run cross validation
set.seed(13)
cv_all_n2 <- fit_resamples(all_n2_final_wf, resamples = folds_all_n2)
cv_all_n2_metrics <- collect_metrics(cv_all_n2,summarize = F)
all_n2_metrics_sum <- collect_metrics(cv_all_n2,summarize = T)

#check cross validation metrics
all_n2_metrics_sum
```

### Null model

```{r}
#create folds for cross validation
set.seed(13)
folds_all_n2_null <- vfold_cv(data_n2_train, 
                        v = nrow(data_n2_train))

#run cross validation
set.seed(13)
cv_all_n2_null <- fit_resamples(all_null_workflow_n2, 
                                resamples = folds_all_n2_null)
cv_all_n2_metrics_null <- collect_metrics(cv_all_n2_null ,summarize = F)
all_n2_metrics_null_sum <- collect_metrics(cv_all_n2_null ,summarize = T)

#check cross validation metrics
all_n2_metrics_null_sum
```

## n = 1

```{r}
#fit model to data
set.seed(13)
fit_all_n1 <- all_n1_final_wf %>% 
  fit(data = data_n1_train)

#create folds for cross validation
set.seed(13)
folds_all_n1 <- vfold_cv(data_n1_train, 
                        v = nrow(data_n1_train))

#run cross validation
set.seed(13)
cv_all_n1 <- fit_resamples(all_n1_final_wf, resamples = folds_all_n1)
cv_all_n1_metrics <- collect_metrics(cv_all_n1,summarize = F)
all_n1_metrics_sum <- collect_metrics(cv_all_n1,summarize = T)

#check cross validation metrics
all_n1_metrics_sum
```

### Null model

```{r}
#create folds for cross validation
set.seed(13)
folds_all_n1_null <- vfold_cv(data_n1_train, 
                        v = nrow(data_n1_train))

#run cross validation
set.seed(13)
cv_all_n1_null <- fit_resamples(all_null_workflow_n1, 
                                resamples = folds_all_n1_null)
cv_all_n1_metrics_null <- collect_metrics(cv_all_n1_null ,summarize = F)
all_n1_metrics_null_sum <- collect_metrics(cv_all_n1_null ,summarize = T)

#check cross validation metrics
all_n1_metrics_null_sum
```

## Export results

```{r}
cv_all_n6_metrics <- cv_all_n6_metrics %>% mutate(.config="all_n6",
                                                  feature="combined")
cv_all_n5_metrics <- cv_all_n5_metrics %>% mutate(.config="all_n5",
                                                  feature="combined")
cv_all_n4_metrics <- cv_all_n4_metrics %>% mutate(.config="all_n4",
                                                  feature="combined")
cv_all_n3_metrics <- cv_all_n3_metrics %>% mutate(.config="all_n3",
                                                  feature="combined")
cv_all_n2_metrics <- cv_all_n2_metrics %>% mutate(.config="all_n2",
                                                  feature="combined")
cv_all_n1_metrics <- cv_all_n1_metrics %>% mutate(.config="all_n1",
                                                  feature="combined")

cv_metrics_all <- rbind(cv_all_n1_metrics,
                       cv_all_n2_metrics,
                       cv_all_n3_metrics,
                       cv_all_n4_metrics,
                       cv_all_n5_metrics,
                       cv_all_n6_metrics)

saveRDS(cv_metrics_all, 
        here("data/processed_data/rand_forest/cv_metrics_all.rds"))
```

# Observed vs predicted

## Training data (2020-2021)

```{r}
#create bootstraps for predictions
set.seed(13)
bootstrap_all_n6 <- bootstraps(data_n6_train, times = 50)

#run bootstrapping and save predictions
bootstrap_all_n6_results <- fit_resamples(all_n6_final_wf, 
        resamples = bootstrap_all_n6,
        control = control_resamples(save_pred = TRUE))

#collect predictions and mutate back to original unit (log10 transformed prior to running model)
predictions_all_n6 <- collect_predictions(bootstrap_all_n6_results, summarize = FALSE) %>% 
mutate(prediction = 10^.pred,
       actual = 10^log10_cases)

predictions_all_n6

#plot observed vs predicted
predictions_all_n6 %>% ggplot(aes(actual, prediction)) +
  geom_point() +
  stat_smooth(method = "lm")
```

```{r}
#calculate 95% confidence interval for predictions
predictions_all_n6_sum <- predictions_all_n6 %>%
  group_by(.row) %>%
  summarize(
    actual = mean(actual),
    prediction_avg = mean(prediction),
    prediction_sd = sd(prediction),
    n = n(),
    error = qt(0.975, df = n-1) * prediction_sd / sqrt(n),
    lower = prediction_avg - error,
    upper = prediction_avg + error) 

#calculate spearman rank correlation
all_n6_cor <- cor(predictions_all_n6_sum$actual,
         predictions_all_n6_sum$prediction_avg,
         method = "spearman")

#plot timeseries
predictions_all_n6_sum %>% 
  ggplot(aes(.row)) +
  geom_line(aes(y=prediction_avg), color = "blue") +
  geom_point(aes(y=prediction_avg), color = "blue") +
  geom_ribbon(aes(ymin=lower, ymax=upper), fill = "blue", alpha = 0.3) +
  geom_line(aes(y=actual), color = "red") +
  geom_point(aes(y=actual), color = "red") +
  ggthemes::theme_clean()
```

## Testing data (2022)

```{r}
#create bootstraps for predictions
set.seed(13)
bootstrap_all_n6_test <- bootstraps(data_n6_test, times = 50)

#run bootstrapping and save predictions
bootstrap_all_n6_results_test <- fit_resamples(all_n6_final_wf, 
        resamples = bootstrap_all_n6_test,
        control = control_resamples(save_pred = TRUE))

#collect predictions and mutate back to original unit (log10 transformed prior to running model)
predictions_all_n6_test <- collect_predictions(bootstrap_all_n6_results_test, summarize = FALSE) %>% 
mutate(prediction = 10^.pred,
       actual = 10^log10_cases)

predictions_all_n6_test

#plot observed vs predicted
predictions_all_n6_test %>% ggplot(aes(actual, prediction)) +
  geom_point() +
  stat_smooth(method = "lm")
```

```{r}
#calculate 95% confidence interval for predictions
predictions_all_n6_sum_test <- predictions_all_n6_test %>%
  group_by(.row) %>%
  summarize(
    actual = mean(actual),
    prediction_avg = mean(prediction),
    prediction_sd = sd(prediction),
    n = n(),
    error = qt(0.975, df = n-1) * prediction_sd / sqrt(n),
    lower = prediction_avg - error,
    upper = prediction_avg + error) 

#calculate spearman rank correlation
all_n6_cor_test <- cor(predictions_all_n6_sum_test$actual,
         predictions_all_n6_sum_test$prediction_avg,
         method = "spearman")

#plot timeseries
predictions_all_n6_sum_test %>% 
  ggplot(aes(.row)) +
  geom_line(aes(y=prediction_avg), color = "blue") +
  geom_point(aes(y=prediction_avg), color = "blue") +
  geom_ribbon(aes(ymin=lower, ymax=upper), fill = "blue", alpha = 0.3) +
  geom_line(aes(y=actual), color = "red") +
  geom_point(aes(y=actual), color = "red") +
  ggthemes::theme_clean()
```

## Full timeseries

```{r}
#create bootstraps for predictions
set.seed(13)
bootstrap_all_n6_full <- bootstraps(data_n6, times = 50)

#run bootstrapping and save predictions
bootstrap_all_n6_results_full <- fit_resamples(all_n6_final_wf, 
        resamples = bootstrap_all_n6_full,
        control = control_resamples(save_pred = TRUE))

#collect predictions and mutate back to original unit (log10 transformed prior to running model)
predictions_all_n6_full <- collect_predictions(bootstrap_all_n6_results_full, summarize = FALSE) %>% 
mutate(prediction = 10^.pred,
       actual = 10^log10_cases)

#calculate 95% confidence interval for predictions
predictions_all_n6_sum_full <- predictions_all_n6_full %>%
  group_by(.row) %>%
  summarize(
    actual = mean(actual),
    prediction_avg = mean(prediction),
    prediction_sd = sd(prediction),
    n = n(),
    error = qt(0.975, df = n-1) * prediction_sd / sqrt(n),
    lower = prediction_avg - error,
    upper = prediction_avg + error) 

#plot timeseries
predictions_all_n6_sum_full %>% 
  ggplot(aes(.row)) +
  geom_line(aes(y=prediction_avg), color = "blue") +
  geom_point(aes(y=prediction_avg), color = "blue") +
  geom_ribbon(aes(ymin=lower, ymax=upper), fill = "blue", alpha = 0.3) +
  geom_line(aes(y=actual), color = "red") +
  geom_point(aes(y=actual), color = "red") +
  ggthemes::theme_clean()
```
